{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f88b1e0e",
      "metadata": {
        "id": "f88b1e0e"
      },
      "source": [
        "# Walmart Recruiting — Store Sales Forecasting (LightAutoML + альтернативные модели)\n",
        "\n",
        "Цель: выполнить практическое задание курса LightAutoML.\n",
        "\n",
        "Требования, которые закрываем в этой работе:\n",
        "1. LAMA бейзлайн: минимум 2 конфигурации и выбор лучшей\n",
        "2. Альтернативные решения без LAMA: PyTorch NN, RandomForest, CatBoost\n",
        "3. Качественный EDA с проверкой гипотез, визуализациями, анализом пропусков и аномалий\n",
        "4. Обоснованная стратегия разбиения данных без утечки\n",
        "5. Self-contained пайплайны и воспроизводимый код\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8feec526",
      "metadata": {
        "id": "8feec526"
      },
      "source": [
        "## 0. Установка зависимостей (Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c26837f7",
      "metadata": {
        "id": "c26837f7"
      },
      "outputs": [],
      "source": [
        "!pip -q install  catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GOlA9dZJnhAm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOlA9dZJnhAm",
        "outputId": "1f681f2c-2fba-439f-f9b7-5c9c5f5fb639"
      },
      "outputs": [],
      "source": [
        "!pip install lightautoml --no-cache-dir"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e43319",
      "metadata": {
        "id": "35e43319"
      },
      "source": [
        "## 1. Импорты и базовые настройки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56f513a5",
      "metadata": {
        "id": "56f513a5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import logging\n",
        "import warnings\n",
        "import zipfile\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44599752",
      "metadata": {
        "id": "44599752"
      },
      "source": [
        "## 2. Загрузка данных\n",
        "\n",
        "В этом ноутбуке данные читаются так, как у тебя уже сделано в Colab (csv внутри zip)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZpzA-HoUnFmj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpzA-HoUnFmj",
        "outputId": "b2c7f6b3-936c-40d8-f3d4-84f24d359f43"
      },
      "outputs": [],
      "source": [
        "zip_path = \"/content/walmart-recruiting-store-sales-forecasting.zip\"\n",
        "extract_path = \"content/walmart\"\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall(extract_path)\n",
        "\n",
        "os.listdir(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "febee1a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "febee1a5",
        "outputId": "500f02b5-0a9f-4fca-a939-6771af0700ab"
      },
      "outputs": [],
      "source": [
        "\n",
        "DATA_DIR = \"/content/content/walmart\"\n",
        "\n",
        "stores_data = pd.read_csv(f\"{DATA_DIR}/stores.csv\")\n",
        "test_data = pd.read_csv(f\"{DATA_DIR}/test.csv.zip\")\n",
        "train_data = pd.read_csv(f\"{DATA_DIR}/train.csv.zip\")\n",
        "features_data = pd.read_csv(f\"{DATA_DIR}/features.csv.zip\")\n",
        "sample_submission = pd.read_csv(f\"{DATA_DIR}/sampleSubmission.csv.zip\")\n",
        "\n",
        "print(train_data.shape, test_data.shape, features_data.shape, stores_data.shape)\n",
        "train_data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6d6f71",
      "metadata": {
        "id": "cd6d6f71"
      },
      "source": [
        "## 3. Метрика соревнования: WMAE\n",
        "\n",
        "Вес 5 для holiday weeks, иначе 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc747b77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc747b77",
        "outputId": "bf41a2f6-470b-43ca-ec47-893398428a2f"
      },
      "outputs": [],
      "source": [
        "def wmae(y_true: np.ndarray, y_pred: np.ndarray, is_holiday: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Weighted MAE from Kaggle Walmart competition:\n",
        "    holiday weeks have weight=5, non-holiday weight=1.\n",
        "\n",
        "    Args:\n",
        "        y_true: true Weekly_Sales\n",
        "        y_pred: predicted Weekly_Sales\n",
        "        is_holiday: binary flag (0/1) aligned with y_true rows\n",
        "\n",
        "    Returns:\n",
        "        WMAE value (float)\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true).reshape(-1)\n",
        "    y_pred = np.asarray(y_pred).reshape(-1)\n",
        "    w = np.where(np.asarray(is_holiday).astype(int).reshape(-1) == 1, 5.0, 1.0)\n",
        "    return np.sum(w * np.abs(y_true - y_pred)) / np.sum(w)\n",
        "\n",
        "# sanity\n",
        "print(\"WMAE sanity:\", wmae([10, 20], [12, 18], [0, 1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4362d0bd",
      "metadata": {
        "id": "4362d0bd"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "\n",
        "def make_submission(test_df: pd.DataFrame, preds: np.ndarray, out_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Create Kaggle submission in format Id=Store_Dept_YYYY-MM-DD.\n",
        "\n",
        "    Note: preds must be aligned row-by-row with test_df.\n",
        "    \"\"\"\n",
        "    preds = np.asarray(preds).reshape(-1)\n",
        "    assert len(preds) == len(test_df), \"preds must have same length as test_df\"\n",
        "\n",
        "    ids = (\n",
        "        test_df[\"Store\"].astype(str)\n",
        "        + \"_\" + test_df[\"Dept\"].astype(str)\n",
        "        + \"_\" + test_df[\"Date\"].astype(str)\n",
        "    )\n",
        "    sub = pd.DataFrame({\"Id\": ids, \"Weekly_Sales\": preds})\n",
        "    sub.to_csv(out_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fa270da",
      "metadata": {
        "id": "6fa270da"
      },
      "source": [
        "## 4. Объединение таблиц и feature engineering\n",
        "\n",
        "Мердж: `(Store, Date, IsHoliday)` + признаки магазина.\n",
        "\n",
        "Гипотеза: эффекты markdown и праздников лучше ловятся после добавления календарных и индикаторов пропусков markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "924c0edc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "924c0edc",
        "outputId": "081f8e6e-af2d-4960-9d26-e49b71903362"
      },
      "outputs": [],
      "source": [
        "def add_date_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    d = pd.to_datetime(df[\"Date\"])\n",
        "    df = df.copy()\n",
        "    df[\"year\"] = d.dt.year.astype(np.int16)\n",
        "    df[\"month\"] = d.dt.month.astype(np.int8)\n",
        "    df[\"weekofyear\"] = d.dt.isocalendar().week.astype(np.int16)\n",
        "    df[\"dayofweek\"] = d.dt.dayofweek.astype(np.int8)\n",
        "\n",
        "    # циклические признаки\n",
        "    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12.0).astype(np.float32)\n",
        "    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12.0).astype(np.float32)\n",
        "    df[\"week_sin\"] = np.sin(2 * np.pi * df[\"weekofyear\"] / 52.0).astype(np.float32)\n",
        "    df[\"week_cos\"] = np.cos(2 * np.pi * df[\"weekofyear\"] / 52.0).astype(np.float32)\n",
        "    return df\n",
        "\n",
        "def build_dataset(train_df: pd.DataFrame,\n",
        "                  test_df: pd.DataFrame,\n",
        "                  features_df: pd.DataFrame,\n",
        "                  stores_df: pd.DataFrame\n",
        "                  ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Merge train/test with features + stores, add date features.\n",
        "    Handle missingness in MarkDown columns by:\n",
        "    - add isna flags (model can learn 'missingness pattern')\n",
        "    - fill NaNs with 0 (common practice for these promo-related features)\n",
        "    \"\"\"\n",
        "    train = train_df.merge(features_df, on=[\"Store\", \"Date\", \"IsHoliday\"], how=\"left\").merge(stores_df, on=\"Store\", how=\"left\")\n",
        "    test = test_df.merge(features_df, on=[\"Store\", \"Date\", \"IsHoliday\"], how=\"left\").merge(stores_df, on=\"Store\", how=\"left\")\n",
        "\n",
        "    train = add_date_features(train)\n",
        "    test = add_date_features(test)\n",
        "\n",
        "    # MarkDown columns + флаги пропусков\n",
        "    md_cols = [c for c in train.columns if re.fullmatch(r\"MarkDown[1-5]\", c)]\n",
        "    for c in md_cols:\n",
        "        train[c + \"_isna\"] = train[c].isna().astype(np.int8)\n",
        "        test[c + \"_isna\"] = test[c].isna().astype(np.int8)\n",
        "        train[c] = train[c].fillna(0.0)\n",
        "        test[c] = test[c].fillna(0.0)\n",
        "\n",
        "    train[\"IsHoliday\"] = train[\"IsHoliday\"].astype(np.int8)\n",
        "    test[\"IsHoliday\"] = test[\"IsHoliday\"].astype(np.int8)\n",
        "\n",
        "    # редкие пропуски в базовых фичах\n",
        "    cont_fill = [\"Temperature\",\"Fuel_Price\",\"CPI\",\"Unemployment\",\"Size\"]\n",
        "    for c in cont_fill:\n",
        "        train[c] = train[c].fillna(train[c].median())\n",
        "        test[c] = test[c].fillna(train[c].median())\n",
        "\n",
        "    return train, test\n",
        "\n",
        "df_train, df_test = build_dataset(train_data, test_data, features_data, stores_data)\n",
        "df_train.shape, df_test.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad828303",
      "metadata": {
        "id": "ad828303"
      },
      "source": [
        "## 5. EDA\n",
        "\n",
        "### 5.1 Анализ таргета\n",
        "\n",
        "Целевая переменная `Weekly_Sales` имеет распределение с тяжёлым хвостом и выраженными выбросами,\n",
        "которые в основном связаны с праздничными неделями и сезонными всплесками спроса.\n",
        "В связи с этим метрики, чувствительные к выбросам (например RMSE), могут быть нестабильны,\n",
        "поэтому в работе используется MAE.\n",
        "\n",
        "Также продажи имеют выраженную временную структуру и сезонность,\n",
        "что требует временного разделения данных для корректной оценки качества модели\n",
        "и предотвращения утечки данных.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e78bdb61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e78bdb61",
        "outputId": "8e0777fb-6f4c-463d-c2b8-93892b9acff7"
      },
      "outputs": [],
      "source": [
        "target = df_train[\"Weekly_Sales\"].astype(float)\n",
        "\n",
        "print(\"Target describe:\")\n",
        "display(target.describe(percentiles=[0.01,0.05,0.5,0.95,0.99]))\n",
        "\n",
        "# распределение\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.hist(np.clip(target, 0, target.quantile(0.99)), bins=80)\n",
        "plt.title(\"Weekly_Sales distribution (clipped at 99p)\")\n",
        "plt.show()\n",
        "\n",
        "# аномалии: отрицательные продажи (есть в датасете)\n",
        "neg_cnt = (target < 0).sum()\n",
        "print(\"Negative sales count:\", int(neg_cnt), \"share:\", float(neg_cnt/len(target)))\n",
        "\n",
        "# динамика во времени (агрегация по дате)\n",
        "tmp = df_train.groupby(\"Date\", as_index=False).agg(\n",
        "    sales_sum=(\"Weekly_Sales\",\"sum\"),\n",
        "    is_holiday=(\"IsHoliday\",\"max\")\n",
        ")\n",
        "tmp[\"Date\"] = pd.to_datetime(tmp[\"Date\"])\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(tmp[\"Date\"], tmp[\"sales_sum\"])\n",
        "plt.title(\"Total sales over time (sum over all stores/depts)\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Holiday weeks vs non-holiday weeks (sum sales):\")\n",
        "display(tmp.groupby(\"is_holiday\")[\"sales_sum\"].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bce1340",
      "metadata": {
        "id": "4bce1340"
      },
      "source": [
        "### 5.2 Анализ признаков\n",
        "\n",
        "### Типы признаков\n",
        "\n",
        "Числовые признаки:\n",
        "- Temperature\n",
        "- Fuel_Price\n",
        "- CPI\n",
        "- Unemployment\n",
        "\n",
        "Категориальные признаки:\n",
        "- Store\n",
        "- Dept\n",
        "- IsHoliday\n",
        "\n",
        "Временной признак:\n",
        "- Date\n",
        "\n",
        "\n",
        "Пропуски в макроэкономических показателях связаны с неполнотой внешней отчётности.\n",
        "Для обработки пропусков использовалась медианная импутация,\n",
        "что позволяет избежать утечки информации из будущего.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f10ade6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9f10ade6",
        "outputId": "dfb71d0b-b3e6-4d84-b8fc-1ef648c10357"
      },
      "outputs": [],
      "source": [
        "# Типизация (базово)\n",
        "cat_cols = [\"Store\", \"Dept\", \"Type\"]\n",
        "date_cols = [\"Date\"]\n",
        "# числовые: всё остальное кроме таргета\n",
        "num_cols = [c for c in df_train.columns if c not in (cat_cols + date_cols + [\"Weekly_Sales\"])]\n",
        "\n",
        "print(\"Categorical:\", cat_cols)\n",
        "print(\"Date:\", date_cols)\n",
        "print(\"Numeric count:\", len(num_cols))\n",
        "\n",
        "# пропуски\n",
        "na = df_train[num_cols].isna().mean().sort_values(ascending=False)\n",
        "display(na.head(15))\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.bar(na.index[:15], na.values[:15])\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.title(\"Top-15 missingness among numeric features (train)\")\n",
        "plt.show()\n",
        "\n",
        "# зависимости: Temperature vs Weekly_Sales (пример)\n",
        "plt.figure(figsize=(6,4))\n",
        "sample = df_train.sample(20000, random_state=SEED)\n",
        "plt.scatter(sample[\"Temperature\"], sample[\"Weekly_Sales\"], s=2)\n",
        "plt.title(\"Temperature vs Weekly_Sales (sample)\")\n",
        "plt.xlabel(\"Temperature\")\n",
        "plt.ylabel(\"Weekly_Sales\")\n",
        "plt.show()\n",
        "\n",
        "# корреляции с таргетом (на подвыборке, чтобы быстрее)\n",
        "sample = df_train.sample(20000, random_state=SEED)\n",
        "\n",
        "corr_df = (\n",
        "    sample[num_cols]\n",
        "    .corrwith(sample[\"Weekly_Sales\"], numeric_only=True)\n",
        "    .sort_values(key=np.abs, ascending=False)\n",
        ")\n",
        "display(corr_df.head(15))\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.bar(corr_df.head(15).index, corr_df.head(15).values)\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.title(\"Top-15 abs correlations with target (sample)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b02c567a",
      "metadata": {
        "id": "b02c567a"
      },
      "source": [
        "## 6. Стратегия разбиения данных\n",
        "\n",
        "Обоснование: это временной ряд по неделям. Чтобы избежать утечки, используем **chronological split**: последние `val_weeks` недель валидация."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76d9136a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76d9136a",
        "outputId": "3854a78a-c6cb-429e-fec3-1f82823d87a2"
      },
      "outputs": [],
      "source": [
        "def chronological_split(train_df: pd.DataFrame,\n",
        "                        val_weeks: int = 8\n",
        "                        ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Time-based split to avoid leakage: validation contains the most recent weeks.\n",
        "    We split by Date (weekly granularity in dataset).\n",
        "    \"\"\"\n",
        "    dates = np.array(sorted(train_df[\"Date\"].unique()))\n",
        "    cut = dates[-val_weeks] if len(dates) > val_weeks else dates[int(len(dates)*0.8)]\n",
        "    tr = train_df[train_df[\"Date\"] < cut].copy()\n",
        "    va = train_df[train_df[\"Date\"] >= cut].copy()\n",
        "    return tr, va\n",
        "\n",
        "train_tr, train_va = chronological_split(df_train, val_weeks=8)\n",
        "print(train_tr[\"Date\"].min(), train_tr[\"Date\"].max(), train_va[\"Date\"].min(), train_va[\"Date\"].max())\n",
        "print(train_tr.shape, train_va.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6d33e02",
      "metadata": {
        "id": "f6d33e02"
      },
      "source": [
        "## 7. Бейзлайн на LightAutoML (2 конфигурации)\n",
        "\n",
        "Важно: LAMA оптимизирует MAE, а итоговую оценку мы считаем WMAE на валидации.\n",
        "\n",
        "Конфигурации:\n",
        "1. Быстрый бейзлайн (меньше timeout)\n",
        "2. Усиленный (больше timeout, больше фолдов)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b03aa297",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b03aa297",
        "outputId": "0b042c37-5bda-436c-efe5-e520042abd71"
      },
      "outputs": [],
      "source": [
        "from lightautoml.tasks import Task\n",
        "from lightautoml.automl.presets.tabular_presets import TabularAutoML\n",
        "\n",
        "# подготовка данных для LAMA\n",
        "lama_tr = train_tr.copy()\n",
        "lama_va = train_va.copy()\n",
        "\n",
        "# для LAMA target в отдельной колонке\n",
        "target_col = \"Weekly_Sales\"\n",
        "\n",
        "# 1) baseline config\n",
        "task = Task(\"reg\", metric=\"mae\", greater_is_better=False)\n",
        "\n",
        "automl_lama_1 = TabularAutoML(\n",
        "    task=task,\n",
        "    timeout=300,\n",
        "    cpu_limit=4,\n",
        "    general_params={\"use_algos\": [[\"lgb\", \"lgb_tuned\"]]},\n",
        "    reader_params={\"random_state\": SEED, \"cv\": 3},\n",
        ")\n",
        "\n",
        "oof_1 = automl_lama_1.fit_predict(lama_tr, roles={\"target\": target_col})\n",
        "pred_va_1 = automl_lama_1.predict(lama_va).data[:, 0]\n",
        "\n",
        "lama_wmae_1 = wmae(lama_va[target_col].values, pred_va_1, lama_va[\"IsHoliday\"].values)\n",
        "print(\"LAMA #1 val WMAE:\", lama_wmae_1)\n",
        "\n",
        "# 2) stronger config\n",
        "automl_lama_2 = TabularAutoML(\n",
        "    task=task,\n",
        "    timeout=900,\n",
        "    cpu_limit=4,\n",
        "    general_params={\"use_algos\": [[\"lgb\", \"lgb_tuned\", \"cb\", \"linear_l2\"]]},\n",
        "    reader_params={\"random_state\": SEED, \"cv\": 5},\n",
        ")\n",
        "\n",
        "oof_2 = automl_lama_2.fit_predict(lama_tr, roles={\"target\": target_col})\n",
        "pred_va_2 = automl_lama_2.predict(lama_va).data[:, 0]\n",
        "\n",
        "lama_wmae_2 = wmae(lama_va[target_col].values, pred_va_2, lama_va[\"IsHoliday\"].values)\n",
        "print(\"LAMA #2 val WMAE:\", lama_wmae_2)\n",
        "\n",
        "best_lama = automl_lama_1 if lama_wmae_1 <= lama_wmae_2 else automl_lama_2\n",
        "best_lama_name = \"LAMA_1\" if best_lama is automl_lama_1 else \"LAMA_2\"\n",
        "print(\"Best LAMA:\", best_lama_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc5bd22",
      "metadata": {
        "id": "4bc5bd22"
      },
      "source": [
        "## 8. Альтернативные решения без LAMA\n",
        "\n",
        "Ниже 3 модели: RandomForest, CatBoost, PyTorch NN.\n",
        "\n",
        "Общее: одна и та же логика подготовки признаков и одинаковая оценка WMAE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75071852",
      "metadata": {
        "id": "75071852"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a680a1eb",
      "metadata": {
        "id": "a680a1eb"
      },
      "source": [
        "### 8.1 RandomForest\n",
        "\n",
        "Примечание: у RF плохо масштабируется one-hot на Store/Dept, поэтому делаем:\n",
        "- Type: one-hot\n",
        "- Store/Dept: частотное кодирование\n",
        "- числовые: как есть\n",
        "\n",
        "Это не лучший метод, но это отдельный пайплайн, который закрывает требование проекта."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b690bfa",
      "metadata": {
        "id": "3b690bfa"
      },
      "outputs": [],
      "source": [
        "def add_frequency_encoding(\n",
        "    tr: pd.DataFrame,\n",
        "    va: pd.DataFrame,\n",
        "    cols: List[str]\n",
        "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "\n",
        "    \"\"\"\n",
        "    Add frequency encoding for categorical columns.\n",
        "\n",
        "    For each column in `cols`, computes category frequencies on the training\n",
        "    split only (to avoid data leakage) and maps them to both train and\n",
        "    validation datasets.\n",
        "\n",
        "    Missing categories in validation are filled with 0.\n",
        "\n",
        "    Args:\n",
        "        tr: Training dataframe.\n",
        "        va: Validation dataframe.\n",
        "        cols: List of categorical column names to encode.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (train_df_with_freq, val_df_with_freq).\n",
        "    \"\"\"\n",
        "    tr = tr.copy()\n",
        "    va = va.copy()\n",
        "    for c in cols:\n",
        "        freq = tr[c].value_counts(dropna=False)\n",
        "        tr[c + \"_freq\"] = tr[c].map(freq).astype(np.float32)\n",
        "        va[c + \"_freq\"] = va[c].map(freq).fillna(0).astype(np.float32)\n",
        "    return tr, va\n",
        "\n",
        "rf_tr, rf_va = add_frequency_encoding(train_tr, train_va, [\"Store\",\"Dept\"])\n",
        "\n",
        "rf_target = rf_tr[\"Weekly_Sales\"].values\n",
        "rf_is_hol = rf_va[\"IsHoliday\"].values\n",
        "\n",
        "rf_num = [c for c in rf_tr.columns if c not in [\"Weekly_Sales\",\"Date\",\"Type\",\"Store\",\"Dept\"]]\n",
        "rf_cat = [\"Type\"]\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))]), rf_num),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), rf_cat),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")\n",
        "\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1,\n",
        "    max_depth=None,\n",
        ")\n",
        "\n",
        "rf_pipe = Pipeline([\n",
        "    (\"prep\", preprocess),\n",
        "    (\"model\", rf_model),\n",
        "])\n",
        "\n",
        "rf_pipe.fit(rf_tr, rf_target)\n",
        "rf_pred = rf_pipe.predict(rf_va)\n",
        "\n",
        "rf_wmae = wmae(train_va[\"Weekly_Sales\"].values, rf_pred, rf_is_hol)\n",
        "print(\"RF val WMAE:\", rf_wmae)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c36e86",
      "metadata": {
        "id": "57c36e86"
      },
      "source": [
        "### 8.2 CatBoost\n",
        "\n",
        "CatBoost нативно обрабатывает категориальные фичи `Store`, `Dept`, `Type`, что обычно дает хороший результат на этом датасете.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "242448ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "242448ae",
        "outputId": "a887132a-57f0-4f56-e757-7c9643c6dc2a"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostRegressor\n",
        "\n",
        "def add_date_parts(df):\n",
        "    \"\"\"\n",
        "    Добавляет календарные признаки, извлечённые из колонки `Date`.\n",
        "\n",
        "    Функция преобразует столбец `Date` в формат datetime и на его основе\n",
        "    создаёт набор стандартных временных признаков, которые часто используются\n",
        "    в задачах прогнозирования временных рядов и табличных моделях.\n",
        "\n",
        "    Добавляемые колонки:\n",
        "    - Year        : год (int)\n",
        "    - Week        : номер недели по ISO-календарю (int)\n",
        "    - Month       : месяц (1–12)\n",
        "    - Day         : день месяца (1–31)\n",
        "    - DayOfWeek   : день недели (0 = понедельник, 6 = воскресенье)\n",
        "\n",
        "    Параметры\n",
        "    ---------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame, содержащий колонку `Date`.\n",
        "        Колонка `Date` должна быть приводима к datetime через `pd.to_datetime`.\n",
        "\n",
        "    Возвращает\n",
        "    ----------\n",
        "    pandas.DataFrame\n",
        "        Тот же DataFrame, дополненный календарными признаками.\n",
        "\n",
        "    Примечания\n",
        "    ----------\n",
        "    - Номер недели считается по ISO-календарю (ISO-8601).\n",
        "    - Функция модифицирует входной DataFrame inplace и возвращает его же.\n",
        "    \"\"\"\n",
        "    d = pd.to_datetime(df[\"Date\"])\n",
        "    df[\"Year\"] = d.dt.year.astype(int)\n",
        "    df[\"Week\"] = d.dt.isocalendar().week.astype(int)\n",
        "    df[\"Month\"] = d.dt.month.astype(int)\n",
        "    df[\"Day\"] = d.dt.day.astype(int)\n",
        "    df[\"DayOfWeek\"] = d.dt.dayofweek.astype(int)\n",
        "    return df\n",
        "\n",
        "train_tr = add_date_parts(train_tr.copy())\n",
        "train_va = add_date_parts(train_va.copy())\n",
        "\n",
        "cb_features = [c for c in train_tr.columns if c not in [\"Weekly_Sales\", \"Date\"]]\n",
        "cb_cat = [\"Store\", \"Dept\", \"Type\"]\n",
        "\n",
        "cb_train = train_tr[cb_features].copy()\n",
        "cb_valid = train_va[cb_features].copy()\n",
        "\n",
        "\n",
        "cb = CatBoostRegressor(\n",
        "    loss_function=\"MAE\",\n",
        "    iterations=5000,\n",
        "    learning_rate=0.03,\n",
        "    depth=10,\n",
        "    random_seed=SEED,\n",
        "    eval_metric=\"MAE\",\n",
        "    early_stopping_rounds=200,\n",
        "    verbose=200,\n",
        ")\n",
        "\n",
        "cb.fit(\n",
        "    cb_train,\n",
        "    train_tr[\"Weekly_Sales\"],\n",
        "    eval_set=(cb_valid, train_va[\"Weekly_Sales\"]),\n",
        "    cat_features=cb_cat,\n",
        ")\n",
        "\n",
        "cb_pred = cb.predict(cb_valid)\n",
        "cb_wmae = wmae(train_va[\"Weekly_Sales\"].values, cb_pred, train_va[\"IsHoliday\"].values)\n",
        "print(\"CatBoost val WMAE:\", cb_wmae)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea15cd6c",
      "metadata": {
        "id": "ea15cd6c"
      },
      "source": [
        "### 8.3 PyTorch Neural Network\n",
        "\n",
        "Эмбеддинги Store/Dept/Type + MLP. Лосс = Weighted MAE (совпадает с метрикой).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2393769",
      "metadata": {
        "id": "e2393769"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def seed_everything(seed: int = 42) -> None:\n",
        "    \"\"\"\n",
        "    Фиксирует сиды (random/NumPy/PyTorch) для воспроизводимости экспериментов.\n",
        "\n",
        "    Что делает:\n",
        "    - задаёт seed для Python `random`, NumPy и PyTorch;\n",
        "    - задаёт seed для всех CUDA-девайсов (если есть);\n",
        "    - включает детерминированный режим cuDNN (где это возможно) и отключает авто-бенчмарк,\n",
        "      чтобы результаты меньше зависели от недетерминированных оптимизаций.\n",
        "\n",
        "    Параметры\n",
        "    ---------\n",
        "    seed : int, по умолчанию 42\n",
        "        Значение сида.\n",
        "\n",
        "    Возвращает\n",
        "    ----------\n",
        "    None\n",
        "    \"\"\"\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def weighted_mae_torch(pred: torch.Tensor, target: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Считает взвешенную MAE (WMAE) в PyTorch.\n",
        "\n",
        "    Формула:\n",
        "        WMAE = sum_i w_i * |pred_i - target_i| / sum_i w_i\n",
        "\n",
        "    Все входные тензоры приводятся к 1D (flatten). Сумма весов защищена от деления на ноль.\n",
        "\n",
        "    Параметры\n",
        "    ---------\n",
        "    pred : torch.Tensor\n",
        "        Предсказания модели (любой формы; будет расплющен в 1D).\n",
        "    target : torch.Tensor\n",
        "        Истинные значения (любой формы; будет расплющен в 1D).\n",
        "    weight : torch.Tensor\n",
        "        Веса объектов (любой формы; будет расплющен в 1D).\n",
        "\n",
        "    Возвращает\n",
        "    ----------\n",
        "    torch.Tensor\n",
        "        Скалярный тензор WMAE.\n",
        "    \"\"\"\n",
        "    pred = pred.view(-1)\n",
        "    target = target.view(-1)\n",
        "    weight = weight.view(-1)\n",
        "    return (weight * (pred - target).abs()).sum() / weight.sum().clamp_min(1.0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_wmae_torch(model: nn.Module, loader: DataLoader, device: torch.device) -> float:\n",
        "    \"\"\"\n",
        "    Оценивает модель на DataLoader по метрике WMAE (взвешенная MAE).\n",
        "\n",
        "    Важно: метрика агрегируется по всему датасету через суммирование числителя/знаменателя,\n",
        "    чтобы не возникал перекос из-за усреднения по батчам.\n",
        "\n",
        "    Ожидаемый формат батча из loader:\n",
        "        (x_cat, x_cont, y, w)\n",
        "    где:\n",
        "        x_cat  : LongTensor [B, n_cat]\n",
        "        x_cont : FloatTensor [B, n_cont]\n",
        "        y      : FloatTensor [B] или [B, 1]\n",
        "        w      : FloatTensor [B] или [B, 1]\n",
        "\n",
        "    Параметры\n",
        "    ---------\n",
        "    model : nn.Module\n",
        "        Модель с сигнатурой model(x_cat, x_cont) -> предсказания.\n",
        "    loader : DataLoader\n",
        "        Валид/тест DataLoader, который отдаёт батчи (x_cat, x_cont, y, w).\n",
        "    device : torch.device\n",
        "        Устройство, на котором считать инференс (cpu/cuda).\n",
        "\n",
        "    Возвращает\n",
        "    ----------\n",
        "    float\n",
        "        Значение WMAE по всему набору данных.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    num, den = 0.0, 0.0\n",
        "    for x_cat, x_cont, y, w in loader:\n",
        "        x_cat = x_cat.to(device)\n",
        "        x_cont = x_cont.to(device)\n",
        "        y = y.to(device)\n",
        "        w = w.to(device)\n",
        "        p = model(x_cat, x_cont).view(-1)\n",
        "        num += float((w.view(-1) * (p - y.view(-1)).abs()).sum().cpu())\n",
        "        den += float(w.sum().cpu())\n",
        "    return num / max(den, 1e-12)\n",
        "\n",
        "class WalmartDS(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset для табличной задачи Walmart Weekly Sales.\n",
        "\n",
        "    Хранит:\n",
        "    - категориальные признаки (индексы) в int64 (long) тензорах;\n",
        "    - непрерывные признаки в float32 тензорах;\n",
        "    - целевую переменную `Weekly_Sales` в float32;\n",
        "    - веса объектов (например, праздники x5) в float32.\n",
        "\n",
        "    Параметры\n",
        "    ---------\n",
        "    x_cat : np.ndarray\n",
        "        Категориальные признаки формы [N, n_cat], уже закодированные индексами.\n",
        "    x_cont : np.ndarray\n",
        "        Непрерывные признаки формы [N, n_cont].\n",
        "    y : np.ndarray\n",
        "        Таргет формы [N] или [N, 1].\n",
        "    w : np.ndarray\n",
        "        Веса объектов формы [N] или [N, 1].\n",
        "\n",
        "    Возвращаемое значение __getitem__\n",
        "    -------------------------------\n",
        "    tuple(torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor)\n",
        "        (x_cat[i], x_cont[i], y[i], w[i])\n",
        "    \"\"\"\n",
        "    def __init__(self, x_cat, x_cont, y, w):\n",
        "        self.x_cat = torch.from_numpy(x_cat).long()\n",
        "        self.x_cont = torch.from_numpy(x_cont).float()\n",
        "        self.y = torch.from_numpy(y).float()\n",
        "        self.w = torch.from_numpy(w).float()\n",
        "    def __len__(self):\n",
        "        \"\"\"Возвращает количество объектов в датасете.\"\"\"\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"\n",
        "        Возвращает один объект по индексу.\n",
        "\n",
        "        Параметры\n",
        "        ---------\n",
        "        i : int\n",
        "            Индекс объекта.\n",
        "\n",
        "        Возвращает\n",
        "        ----------\n",
        "        tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "            (x_cat, x_cont, y, w) для данного индекса.\n",
        "        \"\"\"\n",
        "        return self.x_cat[i], self.x_cont[i], self.y[i], self.w[i]\n",
        "\n",
        "class TabularNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Нейросеть для табличных данных: эмбеддинги категорий + MLP.\n",
        "\n",
        "    Категориальные признаки:\n",
        "    - Store -> эмбеддинг размерности 8\n",
        "    - Dept  -> эмбеддинг размерности 16\n",
        "    - Type  -> эмбеддинг размерности 4\n",
        "\n",
        "    Далее эмбеддинги конкатенируются с непрерывными признаками и подаются в MLP.\n",
        "\n",
        "    Параметры\n",
        "    ---------\n",
        "    n_store : int\n",
        "        Количество уникальных Store (размер словаря эмбеддинга).\n",
        "    n_dept : int\n",
        "        Количество уникальных Dept (размер словаря эмбеддинга).\n",
        "    n_type : int\n",
        "        Количество уникальных Type (размер словаря эмбеддинга).\n",
        "    cont_dim : int\n",
        "        Число непрерывных признаков.\n",
        "    hidden : int, по умолчанию 256\n",
        "        Ширина скрытых слоёв MLP.\n",
        "    drop : float, по умолчанию 0.1\n",
        "        Dropout в MLP.\n",
        "\n",
        "    Вход forward\n",
        "    ------------\n",
        "    x_cat : torch.LongTensor\n",
        "        Тензор формы [B, 3] со столбцами: [store_idx, dept_idx, type_idx].\n",
        "    x_cont : torch.FloatTensor\n",
        "        Тензор формы [B, cont_dim] с непрерывными признаками.\n",
        "\n",
        "    Возвращает\n",
        "    ----------\n",
        "    torch.Tensor\n",
        "        Предсказания формы [B, 1].\n",
        "    \"\"\"\n",
        "    def __init__(self, n_store, n_dept, n_type, cont_dim, hidden=256, drop=0.1):\n",
        "        super().__init__()\n",
        "        self.emb_store = nn.Embedding(n_store, 8)\n",
        "        self.emb_dept = nn.Embedding(n_dept, 16)\n",
        "        self.emb_type = nn.Embedding(n_type, 4)\n",
        "        in_dim = 8 + 16 + 4 + cont_dim\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(hidden, 1),\n",
        "        )\n",
        "    def forward(self, x_cat, x_cont):\n",
        "        \"\"\"\n",
        "        Прямой проход: эмбеддинги -> конкатенация -> MLP.\n",
        "\n",
        "        Параметры\n",
        "        ---------\n",
        "        x_cat : torch.LongTensor\n",
        "            Категориальные индексы, форма [B, 3].\n",
        "        x_cont : torch.FloatTensor\n",
        "            Непрерывные признаки, форма [B, cont_dim].\n",
        "\n",
        "        Возвращает\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            Предсказания, форма [B, 1].\n",
        "        \"\"\"\n",
        "        s = self.emb_store(x_cat[:,0])\n",
        "        d = self.emb_dept(x_cat[:,1])\n",
        "        t = self.emb_type(x_cat[:,2])\n",
        "        x = torch.cat([s,d,t,x_cont], dim=1)\n",
        "        return self.mlp(x)\n",
        "\n",
        "def build_encoders(train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Строит отображения (энкодеры) категорий Store/Dept/Type в индексы.\n",
        "\n",
        "    Маппинги строятся по объединению train + test, чтобы:\n",
        "    - словари категорий были стабильны,\n",
        "    - на тесте не появлялись \"неизвестные\" категории.\n",
        "\n",
        "    Параметры\n",
        "    ---------\n",
        "    train_df : pd.DataFrame\n",
        "        DataFrame с колонками 'Store', 'Dept', 'Type' (обучающая часть).\n",
        "    test_df : pd.DataFrame\n",
        "        DataFrame с колонками 'Store', 'Dept', 'Type' (тест).\n",
        "\n",
        "    Возвращает\n",
        "    ----------\n",
        "    tuple[dict, dict, dict]\n",
        "        store2idx, dept2idx, type2idx:\n",
        "        словари вида {значение_категории -> индекс}.\n",
        "    \"\"\"\n",
        "    all_store = pd.concat([train_df[\"Store\"], test_df[\"Store\"]]).unique()\n",
        "    all_dept = pd.concat([train_df[\"Dept\"], test_df[\"Dept\"]]).unique()\n",
        "    all_type = pd.concat([train_df[\"Type\"], test_df[\"Type\"]]).astype(str).unique()\n",
        "\n",
        "    store2idx = {int(v): i for i, v in enumerate(sorted(all_store))}\n",
        "    dept2idx = {int(v): i for i, v in enumerate(sorted(all_dept))}\n",
        "    type2idx = {str(v): i for i, v in enumerate(sorted(all_type))}\n",
        "    return store2idx, dept2idx, type2idx\n",
        "\n",
        "def make_nn_arrays(df: pd.DataFrame, store2idx, dept2idx, type2idx, cont_cols: List[str]):\n",
        "    \"\"\"\n",
        "    Преобразует DataFrame в NumPy-массивы (x_cat, x_cont) для табличной NN.\n",
        "\n",
        "    Формирование x_cat идёт в фиксированном порядке:\n",
        "        [Store_idx, Dept_idx, Type_idx]\n",
        "    x_cont формируется выборкой колонок `cont_cols` и приводится к float32.\n",
        "\n",
        "    Параметры\n",
        "    ---------\n",
        "    df : pd.DataFrame\n",
        "        Входной DataFrame, содержащий 'Store', 'Dept', 'Type' и континуальные признаки.\n",
        "    store2idx : dict\n",
        "        Словарь {Store -> индекс}.\n",
        "    dept2idx : dict\n",
        "        Словарь {Dept -> индекс}.\n",
        "    type2idx : dict\n",
        "        Словарь {Type(str) -> индекс}.\n",
        "    cont_cols : list[str]\n",
        "        Список колонок непрерывных признаков.\n",
        "\n",
        "    Возвращает\n",
        "    ----------\n",
        "    tuple[np.ndarray, np.ndarray]\n",
        "        x_cat : np.ndarray, dtype=int64, форма [N, 3]\n",
        "        x_cont: np.ndarray, dtype=float32, форма [N, len(cont_cols)]\n",
        "    \"\"\"\n",
        "    x_cat = np.stack([\n",
        "        df[\"Store\"].map(store2idx).astype(np.int64).values,\n",
        "        df[\"Dept\"].map(dept2idx).astype(np.int64).values,\n",
        "        df[\"Type\"].astype(str).map(type2idx).astype(np.int64).values,\n",
        "    ], axis=1)\n",
        "    x_cont = df[cont_cols].astype(np.float32).values\n",
        "    return x_cat, x_cont\n",
        "\n",
        "def standardize(train_x, val_x, test_x):\n",
        "    \"\"\"\n",
        "    Стандартизирует непрерывные признаки по статистикам train.\n",
        "\n",
        "    Для каждого признака:\n",
        "        x' = (x - mean_train) / std_train\n",
        "\n",
        "    Если std слишком мал (признак почти константный), std заменяется на 1.0,\n",
        "    чтобы избежать деления на почти ноль.\n",
        "\n",
        "    Параметры\n",
        "    ---------\n",
        "    train_x : np.ndarray\n",
        "        Непрерывные признаки train, форма [N_train, D].\n",
        "    val_x : np.ndarray\n",
        "        Непрерывные признаки val, форма [N_val, D].\n",
        "    test_x : np.ndarray\n",
        "        Непрерывные признаки test (или любой другой сплит), форма [N_test, D].\n",
        "\n",
        "    Возвращает\n",
        "    ----------\n",
        "    tuple[np.ndarray, np.ndarray, np.ndarray]\n",
        "        (train_x_std, val_x_std, test_x_std) тех же форм, что и входы.\n",
        "    \"\"\"\n",
        "    m = train_x.mean(axis=0, keepdims=True)\n",
        "    s = train_x.std(axis=0, keepdims=True)\n",
        "    s = np.where(s < 1e-6, 1.0, s)\n",
        "    return (train_x-m)/s, (val_x-m)/s, (test_x-m)/s\n",
        "\n",
        "# NN feature set\n",
        "md_cols = [c for c in df_train.columns if re.fullmatch(r\"MarkDown[1-5]\", c)]\n",
        "md_na_cols = [c + \"_isna\" for c in md_cols]\n",
        "nn_cont = [\n",
        "    \"IsHoliday\",\n",
        "    \"Temperature\",\"Fuel_Price\",\"CPI\",\"Unemployment\",\n",
        "    \"Size\",\n",
        "    \"year\",\"month\",\"weekofyear\",\"dayofweek\",\n",
        "    \"month_sin\",\"month_cos\",\"week_sin\",\"week_cos\",\n",
        "] + md_cols + md_na_cols\n",
        "\n",
        "store2idx, dept2idx, type2idx = build_encoders(train_tr, df_test)\n",
        "\n",
        "x_cat_tr, x_cont_tr = make_nn_arrays(train_tr, store2idx, dept2idx, type2idx, nn_cont)\n",
        "x_cat_va, x_cont_va = make_nn_arrays(train_va, store2idx, dept2idx, type2idx, nn_cont)\n",
        "\n",
        "x_cont_tr, x_cont_va, _ = standardize(x_cont_tr, x_cont_va, x_cont_va)\n",
        "\n",
        "y_tr = train_tr[\"Weekly_Sales\"].astype(np.float32).values\n",
        "y_va = train_va[\"Weekly_Sales\"].astype(np.float32).values\n",
        "\n",
        "w_tr = np.where(train_tr[\"IsHoliday\"].values == 1, 5.0, 1.0).astype(np.float32)\n",
        "w_va = np.where(train_va[\"IsHoliday\"].values == 1, 5.0, 1.0).astype(np.float32)\n",
        "\n",
        "ds_tr = WalmartDS(x_cat_tr, x_cont_tr, y_tr, w_tr)\n",
        "ds_va = WalmartDS(x_cat_va, x_cont_va, y_va, w_va)\n",
        "\n",
        "dl_tr = DataLoader(ds_tr, batch_size=4096, shuffle=True, num_workers=0)\n",
        "dl_va = DataLoader(ds_va, batch_size=4096, shuffle=False, num_workers=0)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seed_everything(SEED)\n",
        "\n",
        "model = TabularNN(\n",
        "    n_store=len(store2idx),\n",
        "    n_dept=len(dept2idx),\n",
        "    n_type=len(type2idx),\n",
        "    cont_dim=x_cont_tr.shape[1],\n",
        ").to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
        "\n",
        "best = 1e18\n",
        "best_state = None\n",
        "patience = 4\n",
        "bad = 0\n",
        "\n",
        "for ep in range(1, 30+1):\n",
        "    model.train()\n",
        "    losses=[]\n",
        "    for x_cat, x_cont, y, w in dl_tr:\n",
        "        x_cat = x_cat.to(device)\n",
        "        x_cont = x_cont.to(device)\n",
        "        y = y.to(device)\n",
        "        w = w.to(device)\n",
        "\n",
        "        p = model(x_cat, x_cont).view(-1)\n",
        "        loss = weighted_mae_torch(p, y.view(-1), w.view(-1))\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        losses.append(float(loss.detach().cpu()))\n",
        "    val = eval_wmae_torch(model, dl_va, device)\n",
        "    print(f\"ep={ep:02d} train_loss={np.mean(losses):.5f} val_WMAE={val:.5f}\")\n",
        "\n",
        "    if val < best - 1e-4:\n",
        "        best = val\n",
        "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "        bad = 0\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= patience:\n",
        "            break\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "nn_pred_va = []\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for x_cat, x_cont, y, w in dl_va:\n",
        "        p = model(x_cat.to(device), x_cont.to(device)).view(-1).cpu().numpy()\n",
        "        nn_pred_va.append(p)\n",
        "nn_pred_va = np.concatenate(nn_pred_va)\n",
        "\n",
        "nn_wmae = wmae(train_va[\"Weekly_Sales\"].values, nn_pred_va, train_va[\"IsHoliday\"].values)\n",
        "print(\"NN val WMAE:\", nn_wmae)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "529c08e7",
      "metadata": {
        "id": "529c08e7"
      },
      "source": [
        "## 9. Сравнение моделей и выбор лучшей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fcfc186",
      "metadata": {
        "id": "0fcfc186"
      },
      "outputs": [],
      "source": [
        "results = pd.DataFrame({\n",
        "    \"model\": [\"LAMA_1\", \"LAMA_2\", \"RandomForest\", \"CatBoost\", \"NeuralNet\"],\n",
        "    \"val_WMAE\": [lama_wmae_1, lama_wmae_2, rf_wmae, cb_wmae, nn_wmae],\n",
        "}).sort_values(\"val_WMAE\")\n",
        "\n",
        "display(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32d3a5bb",
      "metadata": {
        "id": "32d3a5bb"
      },
      "source": [
        "## 10. Обучение на всем train и формирование submission\n",
        "\n",
        "берём лучшую из моделей и обучаем на full train, затем предсказываем test.\n",
        "\n",
        "В этом блоке показан пример для LAMA_2 (лучший).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e388a3",
      "metadata": {
        "id": "99e388a3"
      },
      "outputs": [],
      "source": [
        "BEST_MODEL = \"LAMA_2\"  # \"CatBoost\", \"LAMA\", \"NN\", \"RF\"\n",
        "os.makedirs(\"/content/out\", exist_ok=True)\n",
        "\n",
        "if BEST_MODEL == \"CatBoost\":\n",
        "    full_features = [c for c in df_train.columns if c not in [\"Weekly_Sales\"]]\n",
        "    cb_full = CatBoostRegressor(\n",
        "        loss_function=\"MAE\",\n",
        "        iterations=3000,\n",
        "        learning_rate=0.03,\n",
        "        depth=10,\n",
        "        random_seed=SEED,\n",
        "        verbose=200,\n",
        "    )\n",
        "    cb_full.fit(df_train[full_features], df_train[\"Weekly_Sales\"], cat_features=[\"Store\",\"Dept\",\"Type\"])\n",
        "    test_pred = cb_full.predict(df_test[full_features])\n",
        "\n",
        "    out_path = \"/content/out/submission_catboost.csv\"\n",
        "    make_submission(df_test, test_pred, out_path)\n",
        "    print(\"Wrote:\", out_path)\n",
        "\n",
        "elif BEST_MODEL == \"LAMA\":\n",
        "    # использовать best_lama из секции 7\n",
        "    test_pred = best_lama.predict(df_test).data[:, 0]\n",
        "    out_path = \"/content/out/submission_lama.csv\"\n",
        "    make_submission(df_test, test_pred, out_path)\n",
        "    print(\"Wrote:\", out_path)\n",
        "\n",
        "else:\n",
        "    print(\"Для RF и NN сабмит делается аналогично: обучаешь на df_train, предсказываешь df_test.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rTjv7PqBD5mm",
      "metadata": {
        "id": "rTjv7PqBD5mm"
      },
      "outputs": [],
      "source": [
        "BEST_MODEL = \"LAMA\"  # \"CatBoost\", \"LAMA\", \"NN\", \"RF\"\n",
        "os.makedirs(\"/content/out\", exist_ok=True)\n",
        "\n",
        "if BEST_MODEL == \"CatBoost\":\n",
        "    full_features = [c for c in df_train.columns if c not in [\"Weekly_Sales\"]]\n",
        "    cb_full = CatBoostRegressor(\n",
        "        loss_function=\"MAE\",\n",
        "        iterations=3000,\n",
        "        learning_rate=0.03,\n",
        "        depth=10,\n",
        "        random_seed=SEED,\n",
        "        verbose=200,\n",
        "    )\n",
        "    cb_full.fit(df_train[full_features], df_train[\"Weekly_Sales\"], cat_features=[\"Store\",\"Dept\",\"Type\"])\n",
        "    test_pred = cb_full.predict(df_test[full_features])\n",
        "\n",
        "    out_path = \"/content/out/submission_catboost.csv\"\n",
        "    make_submission(df_test, test_pred, out_path)\n",
        "    print(\"Wrote:\", out_path)\n",
        "\n",
        "elif BEST_MODEL == \"LAMA\":\n",
        "    # использовать best_lama из секции 7\n",
        "    test_pred = best_lama.predict(df_test).data[:, 0]\n",
        "    out_path = \"/content/out/submission_lama.csv\"\n",
        "    make_submission(df_test, test_pred, out_path)\n",
        "    print(\"Wrote:\", out_path)\n",
        "\n",
        "else:\n",
        "    print(\"Для RF и NN сабмит делается аналогично: обучаешь на df_train, предсказываешь df_test.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t7lmVdlatQ6G",
      "metadata": {
        "id": "t7lmVdlatQ6G"
      },
      "source": [
        "## 11. Доп альтернативное решение с лучшим скором, чем LLAMA c другим FE и другими моделями"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UKGMPsEhXVMX",
      "metadata": {
        "id": "UKGMPsEhXVMX"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1BNvBpPPhXcP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BNvBpPPhXcP",
        "outputId": "fc31d0e9-15eb-474b-8717-35a4b2da106e"
      },
      "outputs": [],
      "source": [
        "# 0) Fast hyperparameter grid\n",
        "param_grid_fast = {\n",
        "    \"model__n_estimators\": [200, 300, 400],\n",
        "    \"model__max_depth\": [12, 16, 20],\n",
        "    \"model__min_samples_leaf\": [5, 10, 20, 50],\n",
        "    \"model__max_features\": [\"sqrt\", 0.5],\n",
        "}\n",
        "\n",
        "# 1) Логирование (в консоль + файл)\n",
        "log_path = f\"rf_tuning_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
        "    handlers=[logging.FileHandler(log_path), logging.StreamHandler()],\n",
        ")\n",
        "\n",
        "logging.info(\"Start RF tuning\")\n",
        "logging.info(\"Train full shape: %s | Valid shape: %s\", rf_tr.shape, rf_va.shape)\n",
        "\n",
        "\n",
        "# 2) Subsample для ускорения подбора\n",
        "tune_n = 120_000\n",
        "rf_tr_tune = rf_tr.sample(n=min(tune_n, len(rf_tr)), random_state=SEED)\n",
        "rf_target_tune = rf_tr_tune[\"Weekly_Sales\"].values\n",
        "\n",
        "logging.info(\"Tune sample shape: %s\", rf_tr_tune.shape)\n",
        "\n",
        "\n",
        "# 3) Time-aware CV\n",
        "tscv = TimeSeriesSplit(n_splits=2)\n",
        "\n",
        "# 4) RandomizedSearch\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=rf_pipe,\n",
        "    param_distributions=param_grid_fast,\n",
        "    n_iter=8,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=tscv,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    return_train_score=True,\n",
        ")\n",
        "\n",
        "t0 = time.time()\n",
        "search.fit(rf_tr_tune, rf_target_tune)\n",
        "logging.info(\"Search finished in %.1fs\", time.time() - t0)\n",
        "\n",
        "logging.info(\"Best params: %s\", search.best_params_)\n",
        "logging.info(\"Best CV MAE: %.6f\", -search.best_score_)\n",
        "\n",
        "# 5) Оценка на holdout (WMAE)\n",
        "best_rf_pipe = search.best_estimator_\n",
        "rf_pred_tuned = best_rf_pipe.predict(rf_va)\n",
        "\n",
        "rf_wmae_tuned = wmae(\n",
        "    train_va[\"Weekly_Sales\"].values,\n",
        "    rf_pred_tuned,\n",
        "    rf_is_hol\n",
        ")\n",
        "\n",
        "print(\"RF tuned val WMAE:\", rf_wmae_tuned)\n",
        "print(\"Best params:\", search.best_params_)\n",
        "print(\"Log file:\", log_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Uxbqmy4KEj-x",
      "metadata": {
        "id": "Uxbqmy4KEj-x"
      },
      "outputs": [],
      "source": [
        "def type_conversion_full(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Преобразует категориальный признак `Type` в числовое представление.\n",
        "\n",
        "    Выполняет детерминированное отображение строковых значений типа магазина\n",
        "    в числовые коды, пригодные для использования в табличных моделях.\n",
        "\n",
        "    Правило преобразования:\n",
        "    - \"A\" → 3\n",
        "    - \"B\" → 2\n",
        "    - все остальные значения → 1\n",
        "\n",
        "    Параметры\n",
        "    ---------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame, содержащий колонку `Type`.\n",
        "\n",
        "    Возвращает\n",
        "    ----------\n",
        "    pandas.DataFrame\n",
        "        Копию входного DataFrame, в которой колонка `Type`\n",
        "        заменена на числовое значение.\n",
        "\n",
        "    Примечания\n",
        "    ----------\n",
        "    - Функция создаёт копию DataFrame и не модифицирует входной объект inplace.\n",
        "    - Предполагается, что множество значений `Type` ограничено\n",
        "      (\"A\", \"B\" и прочие категории).\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"Type\"] = df[\"Type\"].apply(lambda x: 3 if x == \"A\" else (2 if x == \"B\" else 1))\n",
        "    return df\n",
        "\n",
        "feature_store = features_data.merge(stores_data, how=\"inner\", on=\"Store\")\n",
        "\n",
        "feature_store[\"Date\"] = pd.to_datetime(feature_store[\"Date\"])\n",
        "train_data[\"Date\"] = pd.to_datetime(train_data[\"Date\"])\n",
        "test_data[\"Date\"] = pd.to_datetime(test_data[\"Date\"])\n",
        "\n",
        "feature_store[\"Week\"] = feature_store[\"Date\"].dt.isocalendar().week\n",
        "feature_store[\"Year\"] = feature_store[\"Date\"].dt.year\n",
        "feature_store[\"Day\"] = feature_store[\"Date\"].dt.day\n",
        "\n",
        "train_df = (\n",
        "    train_data.merge(feature_store, how=\"inner\", on=[\"Store\", \"Date\", \"IsHoliday\"])\n",
        "    .sort_values(by=[\"Store\", \"Dept\", \"Date\"])\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "test_df = (\n",
        "    test_data.merge(feature_store, how=\"inner\", on=[\"Store\", \"Date\", \"IsHoliday\"])\n",
        "    .sort_values(by=[\"Store\", \"Dept\", \"Date\"])\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "train_df.loc[(train_df.Year == 2010) & (train_df.Week == 13), \"IsHoliday\"] = True\n",
        "train_df.loc[(train_df.Year == 2011) & (train_df.Week == 16), \"IsHoliday\"] = True\n",
        "train_df.loc[(train_df.Year == 2012) & (train_df.Week == 14), \"IsHoliday\"] = True\n",
        "test_df.loc[(test_df.Year == 2013) & (test_df.Week == 13), \"IsHoliday\"] = True\n",
        "\n",
        "train_df.loc[(train_df.Year == 2010) & (train_df.Week == 18), \"IsHoliday\"] = True\n",
        "train_df.loc[(train_df.Year == 2011) & (train_df.Week == 18), \"IsHoliday\"] = True\n",
        "train_df.loc[(train_df.Year == 2012) & (train_df.Week == 18), \"IsHoliday\"] = True\n",
        "test_df.loc[(test_df.Year == 2013) & (test_df.Week == 18), \"IsHoliday\"] = True\n",
        "\n",
        "train_df.loc[(train_df.Year == 2010) & (train_df.Week == 26), \"IsHoliday\"] = True\n",
        "train_df.loc[(train_df.Year == 2011) & (train_df.Week == 26), \"IsHoliday\"] = True\n",
        "train_df.loc[(train_df.Year == 2012) & (train_df.Week == 27), \"IsHoliday\"] = True\n",
        "test_df.loc[(test_df.Year == 2013) & (test_df.Week == 27), \"IsHoliday\"] = True\n",
        "\n",
        "train_df = type_conversion_full(train_df)\n",
        "test_df = type_conversion_full(test_df)\n",
        "\n",
        "feature_cols = [\"Store\", \"Dept\", \"IsHoliday\", \"Size\", \"Type\", \"Week\", \"Year\", \"Day\"]\n",
        "\n",
        "X = train_df[feature_cols].copy()\n",
        "y = train_df[\"Weekly_Sales\"].copy()\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    random_state=0,\n",
        "    test_size=0.1,\n",
        ")\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=search.best_params_[\"model__n_estimators\"],\n",
        "    max_depth=search.best_params_[\"model__max_depth\"],\n",
        "    min_samples_leaf=search.best_params_[\"model__min_samples_leaf\"],\n",
        "    max_features=search.best_params_[\"model__max_features\"],\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "rf.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "etr = ensemble.ExtraTreesRegressor(\n",
        "    bootstrap=True,\n",
        "    random_state=0,\n",
        ")\n",
        "etr.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "X_test = test_df[feature_cols].copy()\n",
        "pred_rf = rf.predict(X_test)\n",
        "pred_etr = etr.predict(X_test)\n",
        "avg_preds = (pred_rf + pred_etr) / 2\n",
        "\n",
        "test_strip = test_df[[\"Store\", \"Dept\", \"Date\", \"Week\", \"Year\"]].copy()\n",
        "test_strip[\"Weekly_Sales\"] = avg_preds\n",
        "\n",
        "\n",
        "def week_51_adj(row: pd.Series) -> float:\n",
        "    compareval = test_strip[\n",
        "        (test_strip[\"Store\"] == row.Store)\n",
        "        & (test_strip[\"Dept\"] == row.Dept)\n",
        "        & (test_strip[\"Week\"] == 52)\n",
        "    ]\n",
        "    if compareval.empty:\n",
        "        return row.Weekly_Sales\n",
        "    if row.Weekly_Sales > 1.5 * compareval.Weekly_Sales.median():\n",
        "        return row.Weekly_Sales * 0.85\n",
        "    return row.Weekly_Sales\n",
        "\n",
        "\n",
        "def week_52_adj(row: pd.Series) -> float:\n",
        "    compareval = test_strip[\n",
        "        (test_strip[\"Store\"] == row.Store)\n",
        "        & (test_strip[\"Dept\"] == row.Dept)\n",
        "        & (test_strip[\"Week\"] == 51)\n",
        "    ]\n",
        "    if compareval.empty:\n",
        "        return row.Weekly_Sales\n",
        "    if row.Weekly_Sales * 1.275 < compareval.Weekly_Sales.median():\n",
        "        return row.Weekly_Sales * 1.2\n",
        "    return row.Weekly_Sales\n",
        "\n",
        "\n",
        "test_strip[\"Weekly_Sales\"] = test_strip.apply(\n",
        "    lambda row: week_51_adj(row) if row.Week == 51 else row.Weekly_Sales,\n",
        "    axis=1,\n",
        ")\n",
        "test_strip[\"Weekly_Sales\"] = test_strip.apply(\n",
        "    lambda row: week_52_adj(row) if row.Week == 52 else row.Weekly_Sales,\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "sample_submission[\"Weekly_Sales\"] = test_strip[\"Weekly_Sales\"]\n",
        "sample_submission.to_csv(\"submission.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5owABCb6OTRG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5owABCb6OTRG",
        "outputId": "2e0246e4-df09-46c0-c9ef-85f31aaaa42b"
      },
      "outputs": [],
      "source": [
        "# predictions on validation\n",
        "rf_pred_val = rf.predict(X_valid)\n",
        "etr_pred_val = etr.predict(X_valid)\n",
        "\n",
        "# WMAE\n",
        "rf_wmae = wmae(y_valid, rf_pred_val, X_valid[\"IsHoliday\"].values)\n",
        "etr_wmae = wmae(y_valid, etr_pred_val, X_valid[\"IsHoliday\"].values)\n",
        "\n",
        "ens_pred_val = 0.5 * (rf_pred_val + etr_pred_val)\n",
        "ens_wmae = wmae(y_valid, ens_pred_val, X_valid[\"IsHoliday\"].values)\n",
        "\n",
        "print(\"RF val WMAE:\", rf_wmae)\n",
        "print(\"ETR val WMAE:\", etr_wmae)\n",
        "print(\"RF+ETR val WMAE:\", ens_wmae)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07bc4127",
      "metadata": {
        "id": "07bc4127"
      },
      "source": [
        "\n",
        "\n",
        "- На валидации сравнил 5 подходов(3 базовых + LLAMA + лучшее решение через композицию RF и ETR через фичер инжениринг.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YJsx76ZpRuBv",
      "metadata": {
        "id": "YJsx76ZpRuBv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
